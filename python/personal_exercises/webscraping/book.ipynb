{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books ro Scrape\n",
    "#### This notebook scrapes data from books.toscrape.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://books.toscrape.com/'\n",
    "\n",
    "base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(base_url)\n",
    "response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = soup.find_all('article', 'product_pod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = books[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_img_container = book.find('div','image_container')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_url = base_url + book_img_container.a.get('href')\n",
    "book_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_img_url = base_url + book_img_container.img.get('src')\n",
    "book_img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_title = book.h3.a.get('title')\n",
    "book_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_rating = book.p.get('class')[1] + ' stars'\n",
    "book_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_price_container = book.find('div', 'product_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_price = book_price_container.find('p','price_color').text\n",
    "book_price.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_availability = book_price_container.find('p', 'instock').text.strip()\n",
    "book_availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalize extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(book):\n",
    "    book_img_container = book.find('div','image_container')\n",
    "    book_url = base_url + book_img_container.a.get('href')\n",
    "    book_img_url = base_url + book_img_container.img.get('src')\n",
    "\n",
    "    book_title = book.h3.a.get('title')\n",
    "\n",
    "    book_rating = book.p.get('class')[1] + ' stars'\n",
    "\n",
    "    book_price_container = book.find('div', 'product_price')\n",
    "    book_price = book_price_container.find('p','price_color').text\n",
    "    book_availability = book_price_container.find('p', 'instock').text.strip()\n",
    "\n",
    "    return book_url, book_img_url, book_title, book_rating, book_price, book_availability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the next page of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        next = soup.find('li', 'next').a.get('href')\n",
    "        print(next)\n",
    "\n",
    "        if (next.split(\"/\")[0] != 'catalogue'):\n",
    "            next = base_url + 'catalogue/' + next \n",
    "        else: \n",
    "            next = base_url + next\n",
    "        \n",
    "        # print(next)\n",
    "    except AttributeError:\n",
    "        break\n",
    "\n",
    "    \n",
    "    res = requests.get(next)\n",
    "    print('Retrieving ...')\n",
    "    \n",
    "    if (res.status_code == 200):\n",
    "        print('Retrieved: ', next)\n",
    "    else:\n",
    "        print('Error getting url: ', url)\n",
    "        print('Status code: ', res.status_code)\n",
    "\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "\n",
    "    books = soup.find_all('article', 'product_pod')\n",
    "    # len(books)\n",
    "\n",
    "    for book in books:\n",
    "        record = extract_data(book)\n",
    "        records.append(record)\n",
    "\n",
    "len(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_ = soup.find('ul', 'nav').li.ul.find_all('a')\n",
    "category_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = {}\n",
    "for atag in category_:\n",
    "    category_links = 'catalogue/' + atag.get('href')\n",
    "    category_title = atag.text.strip()\n",
    "\n",
    "    if category_title not in category:\n",
    "        category[category_title] = base_url +  category_links\n",
    "\n",
    "category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract data from a category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_data_from_category(category):\n",
    "for cat, url in category.items():\n",
    "    cat_records = []\n",
    "    res = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "    books = soup.find_all('article', 'product_pod')\n",
    "\n",
    "    for book in books:\n",
    "        record = extract_data(book)\n",
    "        cat_records.append(record)\n",
    "\n",
    "    print(cat, cat_records)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def check_url(url):\n",
    "    # if (url.split(\"/\")[0] != 'catalogue'):\n",
    "    #     url = 'catalogue/' + url\n",
    "    \n",
    "    return url.replace('index.html', '')\n",
    "\n",
    "def extract_data(book):\n",
    "    '''Extract data from a book'''\n",
    "    book_img_container = book.find('div','image_container')\n",
    "    book_url = base_url + book_img_container.a.get('href')\n",
    "    book_img_url = base_url + book_img_container.img.get('src')\n",
    "\n",
    "    book_title = book.h3.a.get('title')\n",
    "\n",
    "    book_rating = book.p.get('class')[1] + ' stars'\n",
    "\n",
    "    book_price_container = book.find('div', 'product_price')\n",
    "    book_price = book_price_container.find('p','price_color').text\n",
    "    book_availability = book_price_container.find('p', 'instock').text.strip()\n",
    "\n",
    "\n",
    "    record = (book_url, book_img_url, book_title, book_rating, book_price, book_availability)\n",
    "\n",
    "    return record\n",
    "\n",
    "def main(category):\n",
    "\n",
    "    records = []\n",
    "\n",
    "    page_count = 0\n",
    "    record_count = 0\n",
    "    \n",
    "    for cat,url in category.items():\n",
    "\n",
    "        while True:\n",
    "            res = requests.get(url)\n",
    "\n",
    "            print('Retrieving ...')\n",
    "            \n",
    "            if (res.status_code == 200):\n",
    "                print('Retrieved: ', url)\n",
    "            else:\n",
    "                print('Error getting url: ', url)\n",
    "                print('Status code: ', res.status_code)\n",
    "\n",
    "            page_count += 1\n",
    "\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "            books = soup.find_all('article', 'product_pod')\n",
    "\n",
    "            print('No of books on page: ', len(books))\n",
    "            \n",
    "            print('\\n--------------------------------\\n')\n",
    "\n",
    "            for book in books:\n",
    "                record_count += 1\n",
    "                record_ = extract_data(book)\n",
    "\n",
    "                record = record_ + (cat,)\n",
    "\n",
    "                print('Record: ', record)\n",
    "                print('Records found: ', record_count)\n",
    "                records.append(record)\n",
    "\n",
    "            print('Total no of pages retrieved: ', page_count)\n",
    "           \n",
    "            try:\n",
    "                next = soup.find('li', 'next').a.get('href')\n",
    "                print(next)\n",
    "\n",
    "                url = url + check_url(next)\n",
    "                print('Getting next page: ', url)\n",
    "            except AttributeError:\n",
    "                print('No more pages found')\n",
    "                print('\\n--------------------------------\\n')\n",
    "                break\n",
    "\n",
    "            if page_count % 5   == 0:\n",
    "                print('Pausing...')\n",
    "                time.sleep(2)\n",
    "\n",
    "        print('Writing to file......')\n",
    "        with open('./data/books_to_scrape.csv', 'w') as f:\n",
    "            fieldnames = ['url', 'img_url', 'title', 'rating', 'price', 'availability', 'category']\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(fieldnames)\n",
    "            writer.writerows(records)\n",
    "\n",
    "\n",
    "base_url = 'https://books.toscrape.com/'\n",
    "\n",
    "response = requests.get(base_url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "category_ = soup.find('ul', 'nav').li.ul.find_all('a')\n",
    "\n",
    "category = {}\n",
    "\n",
    "for atag in category_:\n",
    "    category_links = base_url + check_url(atag.get('href'))\n",
    "    category_title = atag.text.strip()\n",
    "\n",
    "    if category_title not in category:\n",
    "        category[category_title] = category_links\n",
    "\n",
    "main(category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
